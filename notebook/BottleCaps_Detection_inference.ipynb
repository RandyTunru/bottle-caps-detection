{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14c2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics onnxruntime numpy opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5faadf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afadc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PT_PATH = '../model/best.pt'\n",
    "\n",
    "DATASET_YAML_PATH = '../bottle_cap_dataset/dataset.yaml'\n",
    "\n",
    "MODEL_ONNX_PATH = '../model/best_int8.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.228  Python-3.10.19 torch-2.2.2+cu121 CPU (AMD Ryzen 9 5900HS with Radeon Graphics)\n",
      "WARNING half=True only compatible with GPU export, i.e. use device=0, setting half=False.\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '..\\model\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 7, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.74...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  7.4s, saved as '..\\model\\best.onnx' (10.1 MB)\n",
      "\n",
      "Export complete (8.9s)\n",
      "Results saved to \u001b[1mC:\\Users\\randy\\Development\\Computer Vision Projects\\bottle_cap_detection\\model\u001b[0m\n",
      "Predict:         yolo predict task=detect model=..\\model\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=..\\model\\best.onnx imgsz=640 data=bottle_cap_dataset/dataset.yaml  \n",
      "Visualize:       https://netron.app\n",
      "\n",
      "An error occurred during export: [WinError 183] Cannot create a file when that file already exists: '../model/best.onnx' -> '../model/quantizedfp16.onnx'\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(MODEL_PT_PATH)\n",
    "\n",
    "try:\n",
    "    model.export(\n",
    "        format='onnx',\n",
    "        imgsz=640,\n",
    "        device='cpu',\n",
    "        dynamic=True,\n",
    "        simplify=True,\n",
    "        opset=12,\n",
    "    )\n",
    "    exported_original_path = MODEL_PT_PATH.replace('.pt', '.onnx')\n",
    "    os.rename(exported_original_path, MODEL_ONNX_PATH)\n",
    "    \n",
    "    print(f\"\\nSuccessfully exported and saved to: {MODEL_ONNX_PATH}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16795307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input Shape: ['batch', 3, 'height', 'width']\n",
      "Loading and pre-processing test image...\n",
      "Running warm-up (20 inferences)\n",
      "Running benchmark (100 inferences)\n",
      "\n",
      "ONNX Runtime Benchmark Results\n",
      "Total time for 100 inferences: 17.49 seconds\n",
      "Average inference time: 174.88 ms\n",
      "Frames Per Second (FPS): 5.72\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    session = ort.InferenceSession(MODEL_ONNX_PATH, providers=['CPUExecutionProvider']) \n",
    "\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_shape = session.get_inputs()[0].shape  \n",
    "    input_height = 640\n",
    "    input_width = 640\n",
    "    \n",
    "    print(f\"Model Input Shape: {input_shape}\")\n",
    "\n",
    "    print(\"Loading and pre-processing test image...\")\n",
    "    test_image_path = \"../bottle_cap_dataset/images/val/raw-250110_dc_s001_b2_3.jpg\" \n",
    "    \n",
    "    if not os.path.exists(test_image_path):\n",
    "        print(f\"ERROR: Test image not found at {test_image_path}\")\n",
    "    else:\n",
    "        image = cv2.imread(test_image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        resized = cv2.resize(image_rgb, (input_width, input_height))\n",
    "        \n",
    "        preprocessed_image = (resized / 255.0).transpose(2, 0, 1).astype(np.float32)\n",
    "        \n",
    "        # Add a batch dimension (B, C, H, W)\n",
    "        input_tensor = np.expand_dims(preprocessed_image, axis=0)\n",
    "\n",
    "        print(\"Running warm-up (20 inferences)\")\n",
    "        for _ in range(20):\n",
    "            session.run(None, {input_name: input_tensor})\n",
    "\n",
    "        num_runs = 100\n",
    "        print(f\"Running benchmark ({num_runs} inferences)\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            session.run(None, {input_name: input_tensor})\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time = end_time - start_time\n",
    "        avg_time_ms = (total_time / num_runs) * 1000\n",
    "        fps = 1000 / avg_time_ms\n",
    "\n",
    "        print(\"\\nONNX Runtime Benchmark Results\")\n",
    "        print(f\"Total time for {num_runs} inferences: {total_time:.2f} seconds\")\n",
    "        print(f\"Average inference time: {avg_time_ms:.2f} ms\")\n",
    "        print(f\"Frames Per Second (FPS): {fps:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during ONNX benchmark: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99ea14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import TensorProto\n",
    "\n",
    "def check_quantization(model_path):\n",
    "    print(f\"Inspecting model: {model_path}\")\n",
    "    \n",
    "    model = onnx.load(model_path)\n",
    "    graph = model.graph\n",
    "\n",
    "    total_weights = len(graph.initializer)\n",
    "    int8_weights = 0\n",
    "    float_weights = 0\n",
    "    \n",
    "    print(f\"Checking {total_weights} Weight Tensors\")\n",
    "    \n",
    "    for tensor in graph.initializer:\n",
    "        if tensor.data_type == TensorProto.INT8:\n",
    "            int8_weights += 1\n",
    "        elif tensor.data_type == TensorProto.UINT8:\n",
    "            int8_weights += 1\n",
    "        elif tensor.data_type == TensorProto.FLOAT:\n",
    "            float_weights += 1\n",
    "\n",
    "    print(f\"INT8/UINT8 Weights: {int8_weights}\")\n",
    "    print(f\"FLOAT32 Weights:    {float_weights}\")\n",
    "\n",
    "    q_nodes = [n for n in graph.node if n.op_type == \"QuantizeLinear\"]\n",
    "    dq_nodes = [n for n in graph.node if n.op_type == \"DequantizeLinear\"]\n",
    "\n",
    "    print(f\"\\nChecking Architecture (QDQ Nodes)\")\n",
    "    print(f\"QuantizeLinear Nodes:   {len(q_nodes)}\")\n",
    "    print(f\"DequantizeLinear Nodes: {len(dq_nodes)}\")\n",
    "    \n",
    "    if int8_weights > 0 and len(q_nodes) > 0:\n",
    "        print(\"\\nThis model contains quantized INT8 weights and operators.\")\n",
    "    else:\n",
    "        print(\"\\nThis model appears to be FP32 (Unquantized).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e652f10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking FP32 model\n",
      "Inspecting model: ../model/best_fp32.onnx\n",
      "Checking 199 Weight Tensors\n",
      "INT8/UINT8 Weights: 0\n",
      "FLOAT32 Weights:    185\n",
      "\n",
      "Checking Architecture (QDQ Nodes)\n",
      "QuantizeLinear Nodes:   0\n",
      "DequantizeLinear Nodes: 0\n",
      "\n",
      "This model appears to be FP32 (Unquantized).\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking FP32 model\")\n",
    "check_quantization(\"../model/best_fp32.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b2938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking INT8 model\n",
      "Inspecting model: ../model/best_int8.onnx\n",
      "Checking 897 Weight Tensors\n",
      "INT8/UINT8 Weights: 350\n",
      "FLOAT32 Weights:    359\n",
      "\n",
      "Checking Architecture (QDQ Nodes)\n",
      "QuantizeLinear Nodes:   174\n",
      "DequantizeLinear Nodes: 349\n",
      "\n",
      "This model contains quantized INT8 weights and operators.\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking INT8 model\")\n",
    "check_quantization(\"../model/best_int8.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
